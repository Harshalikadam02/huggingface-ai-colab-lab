# Hugging Face AI Colab Lab ğŸš€

Hosting and running a large language model (LLM) locally is not always easy â€” GPU constraints, token limits, and setup overhead can make it a big task.  

This repo is my experiment in streamlining that process. Instead of just running a model, I focused on **simplifying the pipeline of tokenization â†’ generation â†’ detokenization**. Itâ€™s both fun and educational to see how text flows through the model in a more transparent way.  

## ğŸ“‚ Included Notebooks
- `gemma_textgen_demo.ipynb` â†’ First working demo of running **Gemma** locally in Colab with cleaned outputs.  
- `gemma_textgen_02.ipynb` â†’ An improved version where the tokenization pipeline is highlighted and automated.

## â–¶ï¸ Try it Yourself
Click below to run the notebooks in Colab:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Harshalikadam02/huggingface-ai-colab-lab/blob/main/gemma_textgen_demo.ipynb)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Harshalikadam02/huggingface-ai-colab-lab/blob/main/gemma_textgen_02.ipynb)

---

âœ¨ The goal isnâ€™t just â€œmaking it runâ€ but making the workflow **clear, modular, and reproducible**. Tokenization might sound small, but itâ€™s where the LLM magic begins.
